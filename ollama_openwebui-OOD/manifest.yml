---
name: "Ollama (Open WebUI)"
category: "Interactive Apps"
subcategory: "AI"
role: "batch_connect"
description: "Launch Open WebUI connected to an Ollama server for local LLM inference. We recommend requesting 8-16 cores along with the following Advanced options: memory=64G, and at least a A100 GPU and higher depending on your model memory size."
