#!/usr/bin/env bash

################################################################################
# Environment Modules
################################################################################

# module load <%#= context.python %>
# additional environment
<%- if context.additional_environment != "" -%>
  <%= context.additional_environment.gsub /\r\n?/, "\n" %>
<%- end -%>
module load apptainer/1.4.1
mkdir -p <%= context.scratch_dir %>

# launch Ollama - theoretically we don't need the & at the end since this is a separate job now.
#./start_vllm_server.sh $port $MODEL $SCRATCH_DIR $HUGGINGFACE_TOKEN >& llm.log 
#--host=should be IP
if [ -e ${ollama_models}${ollama_model}/chat_template.jinja ]; then
	apptainer run --nv -e --fakeroot --env HF_HOME=/root/.cache/ --bind <%= context.scratch_dir %>:/root/.cache  /uufs/chpc.utah.edu/sys/installdir/r8/vllm/vllm_current_cuda_sif_link ${ollama_models}${ollama_model} --host=0.0.0.0 --port=$port --chat-template ${ollama_models}${ollama_model}/chat_template.jinja >& llm.log
else
	apptainer run --nv -e --fakeroot --env HF_HOME=/root/.cache/ --bind <%= context.scratch_dir %>:/root/.cache  /uufs/chpc.utah.edu/sys/installdir/r8/vllm/vllm_current_cuda_sif_link ${ollama_models}${ollama_model} --host=0.0.0.0 --port=$port >& llm.log
fi

# apptainer run --nv -e --fakeroot --env HF_HOME=/root/.cache/ --bind <%= context.scratch_dir %>:/root/.cache  /uufs/chpc.utah.edu/sys/installdir/r8/vllm/vllm_current_cuda_sif_link --host=0.0.0.0 --port=$port --model=${ollama_models}${ollama_model} --task= context.task_widget  >& llm.log


