{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65db4aff-72a2-4cf9-9865-6c15d4a3aad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Check - # of devices= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1081: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d069500b83a44e399ef4ba05ecd7ef8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] =  f\"{os.environ['HOME']}/llm/cache\"\n",
    "import torch\n",
    "print('CUDA Check - # of devices=',torch.cuda.device_count())\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, AutoConfig\n",
    "model_id = f\"{os.environ['LLM_CACHE_PATH']}/{os.environ['MODEL']}\"      # /meta-llama/Llama-2-7b-chat-hf/\"\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True, use_auth_token=True)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_id)\n",
    "model = LlamaForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, load_in_8bit=True, device_map=\"auto\")\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e33a1857-88df-4f0d-af05-7677507622b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/scratch/local/u6040150/4410374/ipykernel_353572/3668508690.py:14: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  local_llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=2048,\n",
    "    # temperature=1,\n",
    "    # top_p=0.95,\n",
    "    # repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory#Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e1a180-0543-4786-a9d5-82338b88617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/u6040150/4410374/ipykernel_353572/1944454211.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(llm=local_llm, max_token_limit=512)\n",
      "/scratch/local/u6040150/4410374/ipykernel_353572/1944454211.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=local_llm, max_token_limit=512)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "conversation = ConversationChain(\n",
    "    llm=local_llm,\n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "conversation.prompt.template='''[INST]<<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
    "<</SYS>>\n",
    "\n",
    "CONTEXT:\n",
    "\n",
    "{history}\n",
    "\n",
    "Question: {input}[/INST]'''\n",
    "\n",
    "class ChatBot:\n",
    "    exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n",
    "\n",
    "    #Method to start the conversation\n",
    "    def start_chat(self):\n",
    "        user_response = input(\"Chat here!\\n\")\n",
    "        while user_response == '':\n",
    "            user_response = input(\"Chat here!\\n\")\n",
    "        self.chat(user_response)\n",
    "\n",
    "    #Method to handle the conversation\n",
    "    def chat(self, reply):\n",
    "        while not self.make_exit(reply):\n",
    "            input_ = reply\n",
    "            reply = input(f\"{conversation.predict(input = input_)}\\n\")\n",
    "\n",
    "    #Method to check for exit commands\n",
    "    def make_exit(self, reply):\n",
    "        for exit_command in self.exit_commands:\n",
    "            if exit_command in reply.lower():\n",
    "                memory.clear()\n",
    "                torch.cuda.empty_cache()\n",
    "                print(\"Ok, have a great day!\")\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d4ff8-99c1-40c5-b0df-370c1ec554b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Chat here!\n",
      " what sound does a duck make?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/conda/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[INST]<<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "CONTEXT:\n",
      "\n",
      "Human: Hello\n",
      "AI: What's up\n",
      "\n",
      "Question: what sound does a duck make?[/INST]  A duck makes the sound \"quack.\"\n",
      " what about penguins?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "chatbot = ChatBot()\n",
    "chatbot.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f371e15-d5f7-4b5f-84d2-ef5bc1497e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
