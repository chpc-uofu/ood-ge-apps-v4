<%
app_name = "vLLM Microservice"
template_root = "/var/www/ood/apps/templates/"
gpu_data = {
  gpu_partitions: CustomGPUPartitions.gpu_partitions,
  gpu_name_mappings: CustomGPUMappings.gpu_name_mappings
}
%>
---
title: <%= app_name %>
attributes:
  gpudata:
    widget: hidden_field
    cacheable: false
    value: |
      "<%= gpu_data.to_json %>"
  llm:
    widget: select
    label: Large Language Model Servers (LLM)
    help: "Select the microservice you want to run."
    # I should be able to use the two fields here
    options:
#      - [ "ollama server", "ollama server"
#      ]
#      - [ "ollama", "ollama"
#      ]
      - [ "vLLM", "vLLM"
        ]
  
  llmmodel:
    widget: select
    label: Specific LLM to run from our local repository
    help: Select the LLM model you want to run
    options:
      - ["meta-llama/Llama-4-Scout-17B-16E-Instruct"]
      - ["meta-llama/Llama-4-Scout-17B-16E"]
      - ["meta-llama/Llama-3.3-70B-Instruct"]
      - ["meta-llama/Llama-3.1-8B"]
      - ["meta-llama/Llama-3.1-8B-Instruct"]
      - ["CohereLabs/aya-expanse-8b"]
      - ["CohereLabs/aya-expanse-32b"]
      - ["google/gemma-3-27b-it"]
      - ["google/gemma-3-1b-it"]
      - ["google/gemma-3-4b-it"]
      - ["Qwen/Qwen3-0.6B"]
      - ["Qwen/Qwen3-1.7B"]
      - ["Qwen/Qwen3-4B"]
      - ["Qwen/Qwen3-8B"]
      - ["Qwen/Qwen3-14B"]
      - ["Qwen/Qwen3-32B"]
      - ["Qwen/Qwen2.5-VL-7B-Instruct"]
      - ["Qwen/Qwen2.5-VL-32B-Instruct"]
      - ["Qwen/Qwen2.5-VL-72B-Instruct"]
      - ["allenai/OLMo-2-0425-1B-Instruct"]
      - ["allenai/OLMo-2-1124-7B-Instruct"]
      - ["allenai/OLMo-2-1124-13B-Instruct"]
      - ["allenai/OLMo-2-0325-32B-Instruct"]
      - ["deepseek-ai/DeepSeek-R1"]
      - ["deepseek-ai/DeepSeek-V3"]
      - ["openai/gpt-oss-20b"]
    required: true

  scratch_dir:
    widget: "path_selector"
    label: Path to user scratch directory
    help: "Enter path to user-specific scratch directory e.g. /scratch/general/vast/$USER/vllm_cache.
    This directory will be your cache directory so models you previously used can be reused without redownloading them,
           <b>NOTE</b>: If the VLLM container does not start, look into the llm.log file in the job's staging directory and contact helpdesk@chpc.utah.edu for more help."
    directory:  /scratch/general/vast/<%= CurrentUser.name %>/vllm_cache
    value:  /scratch/general/vast/<%= CurrentUser.name %>/vllm_cache
    readonly: false
    show_hidden: false
    cacheable: false 
    # favorites:
    # - "<%= CurrentUser.home %>"

  huggingfacetoken:
    widget: text_field
    label: Huggingface token (required for some models)
    help: Check SessionID and then llm.log if your model fails to start

  interface:
    widget: select
    label: <%= app_name %> interface
    help: "This defines the interface of <%= app_name %> you want to start (Notebook or Lab)."
    options:
      - [ "Notebook", "notebook" ]
      - [ "Lab", "lab" ]
  python:
    widget: select
    label: <%= app_name %> Python version
    help: "This defines the Python distribution of <%= app_name %> you want to start."
    options:
      - [  "CHPC Python 3.11.3", "python/3.11.3",
        ]
  cluster:
    widget: select
    options:
      <%- CustomQueues.clusters.each do |g| %>
      - "<%= g %>"
      <%- end %>
<% IO.foreach(template_root+"friscos_v2") do |line| %>
<%= line %>
<% end %>    
<% IO.foreach(template_root+"cluster-help") do |line| %>
<%= line %>
<% end %>    
  custom_accpart:
    widget: select
    options:
      <%- CustomAccPart.accpart.each do |g| %>
      - "<%= g %>"
      <%- end %>
<% IO.foreach(template_root+"accpart-help") do |line| %>
<%= line %>
<% end %>    

<% IO.foreach(template_root+"job_params_v33") do |line| %>
<%= line %>
<% end %>   

form:
  - llm
  - llmmodel
  - scratch_dir
  - huggingfacetoken
  - interface
  - python
<% IO.foreach(template_root+"form_params_gpu") do |line| %>
<%= line %>
<% end %>   
